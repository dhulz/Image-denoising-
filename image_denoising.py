# -*- coding: utf-8 -*-
"""image denoising

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vbFiPwu1Pf2xjXavDimsaaHabmz3iN8a
"""



from google.colab import drive
drive.mount('/content/drive')

import zipfile
zip_ref = zipfile.ZipFile( '/content/drive/MyDrive/denoising dataset.zip', 'r')
zip_ref.extractall("/content/dataset")
zip_ref.close()

import os
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, losses, Model
from PIL import Image

def image_generator(folder, batch_size=4):
    while True:
        images = []
        for filename in os.listdir(folder):
            img_path = os.path.join(folder, filename)
            if img_path.endswith(".png") or img_path.endswith(".jpg"):
                img = Image.open(img_path).convert('L')  # Convert to grayscale
                img = np.array(img).astype('float32') / 255.0  # Normalize
                images.append(img)
                if len(images) == batch_size:
                    yield np.expand_dims(np.array(images), axis=-1), np.expand_dims(np.array(images), axis=-1)
                    images = []
        if len(images) > 0:
            yield np.expand_dims(np.array(images), axis=-1), np.expand_dims(np.array(images), axis=-1)

# Define PSNR function
def psnr(y_true, y_pred):
    max_pixel = 1.0  # Assuming images are normalized to [0, 1]
    mse = tf.keras.backend.mean(tf.keras.backend.square(y_true - y_pred))
    return 20 * tf.keras.backend.log(max_pixel / tf.keras.backend.sqrt(mse)) / tf.keras.backend.log(10.0)

# Paths to your dataset
train_noisy_folder = '/content/dataset/denoising dataset/image denoising/train/noisy'
train_clean_folder = '/content/dataset/denoising dataset/image denoising/train/clean'
test_noisy_folder = '/content/dataset/denoising dataset/image denoising/test/noisy'
test_clean_folder = '/content/dataset/denoising dataset/image denoising/test/clean'

# Create image data generators
batch_size = 32
train_generator = image_generator(train_noisy_folder, batch_size=batch_size)
test_generator = image_generator(test_noisy_folder, batch_size=batch_size)

# Define U-Net model
def unet_model(input_shape):
    inputs = tf.keras.Input(shape=input_shape)

    # Downsample blocks
    conv1 = layers.Conv2D(32, 3, activation='tanh', padding='same')(inputs)
    conv1 = layers.Conv2D(32, 3, activation='tanh', padding='same')(conv1)
    pool1 = layers.MaxPooling2D(pool_size=(2, 2))(conv1)

    conv2 = layers.Conv2D(64, 3, activation='tanh', padding='same')(pool1)
    conv2 = layers.Conv2D(64, 3, activation='tanh', padding='same')(conv2)
    pool2 = layers.MaxPooling2D(pool_size=(2, 2))(conv2)

    # Upsample blocks
    up1 = layers.Conv2DTranspose(32, 2, strides=(2, 2), padding='same')(pool2)
    up1 = layers.concatenate([up1, conv2], axis=3)
    conv3 = layers.Conv2D(32, 3, activation='tanh', padding='same')(up1)
    conv3 = layers.Conv2D(32, 3, activation='tanh', padding='same')(conv3)

    up2 = layers.Conv2DTranspose(16, 2, strides=(2, 2), padding='same')(conv3)
    up2 = layers.concatenate([up2, conv1], axis=3)
    conv4 = layers.Conv2D(16, 3, activation='tanh', padding='same')(up2)
    conv4 = layers.Conv2D(16, 3, activation='tanh', padding='same')(conv4)

    outputs = layers.Conv2D(1, 1, activation='tanh')(conv4)

    model = tf.keras.Model(inputs=inputs, outputs=outputs)
    return model

# Instantiate the U-Net model
unet = unet_model((None, None, 1))

# Compile the model
unet.compile(optimizer='adam', loss=losses.MeanSquaredError(), metrics=[psnr])

# Training the model using generators
history = unet.fit(train_generator,
                   steps_per_epoch=12,
                   epochs=50,
                   validation_data=test_generator,
                   validation_steps=6)

# Evaluate the model
test_loss, test_psnr = unet.evaluate(test_generator, steps=75)
print(f"Test Loss: {test_loss:.4f}, Test PSNR: {test_psnr:.2f} dB")

